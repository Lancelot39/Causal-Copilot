User original query (TOP PRIORITY):
Do causal discovery on this dataset

The computation has to be finished in the runtime of 1440.0 minutes.

## ⚠️ ESSENTIAL QUERY PRIORITY ⚠️
- User query SUPERSEDES all standard hyperparameter guidelines
- Extract specific needs, constraints, domain insights from user query FIRST
- Parameters MUST be adjusted to meet user's explicit requirements 
- ALL recommendations MUST directly align with the user's stated objectives
- User domain knowledge overrides general optimization guidelines

-----------------------------------------------
Given a dataset with the following properties:

1. Columns: Age	Length	Shell weight	Diameter	Height	Whole weight	Shucked weight	Viscera weight
2. Statistics:
The dataset has the following characteristics:

Data Type: The overall data type is Time-series.

The sample size is 4177 with 8 features. 

This dataset is time-series data. 

Data Quality: There are no missing values in the dataset.

Statistical Properties:
- Linearity: The relationships between variables are linear.
- Gaussian Errors: The errors in the data do not follow a Gaussian distribution.
- Time lag: 1 

- Stationarity: The dataset is stationary. 




3. Background Knowledge:
Certainly! Below is a comprehensive analysis of the Abalone dataset based on domain knowledge:

### 1. VARIABLE DESCRIPTIONS

- **Age**: Represents the age of the abalone, typically estimated by counting the number of rings on the shell and adding a constant (usually 1.5) to account for the initial growth period. Measured in years.
- **Length**: The longest shell measurement, typically in millimeters.
- **Shell weight**: The weight of the shell after drying, measured in grams.
- **Diameter**: The measurement perpendicular to the length, also in millimeters.
- **Height**: The height with the meat in the shell, measured in millimeters.
- **Whole weight**: The weight of the whole abalone, measured in grams.
- **Shucked weight**: The weight of the abalone meat, measured in grams.
- **Viscera weight**: The weight of the gut, measured in grams.

### 2. CAUSAL RELATIONSHIPS

- **Direct Causal Relationships**: 
  - Age is likely to directly influence Length, Diameter, and Weight variables as abalones grow over time.
  - Length and Diameter are likely to directly influence Whole weight, Shucked weight, and Shell weight.
- **Indirect Causal Relationships**:
  - Age may indirectly affect Shell weight through its influence on Length and Diameter.

### 3. RELATIONSHIP NATURE

- Relationships are likely to be nonlinear, especially between Age and physical measurements like Length and Weight, due to biological growth patterns.

### 4. DATA DISTRIBUTION

- **Age**: Typically follows a right-skewed distribution as younger abalones are more common.
- **Length, Diameter, Height**: Likely to follow a normal distribution due to natural variation in size.
- **Weight Variables**: May exhibit right-skewed distributions due to the presence of a few very large abalones.

### 5. CONFOUNDERS

- Environmental factors such as water temperature, food availability, and habitat conditions could confound relationships between Age and physical measurements.

### 6. TEMPORAL ASPECTS

- Not directly applicable as the dataset is cross-sectional, but Age serves as a proxy for time.

### 7. HETEROGENEITY

- Different species or populations of abalone might exhibit different growth patterns, affecting the relationships between variables.

### 8. GRAPH DENSITY

- Likely sparse, as biological growth processes tend to have a few key influencing factors.

### 9. DOMAIN-SPECIFIC CONSTRAINTS

- Physical constraints such as the maximum possible size of an abalone shell or weight.

### 10. RELEVANT LITERATURE

- Studies on abalone growth and aquaculture provide insights into typical growth patterns and influencing factors.

### 11. DATA QUALITY ISSUES

- Measurement errors in physical dimensions and weights are possible. Missing data might occur if some measurements were not taken.

### 12. INTERACTION EFFECTS

- Potential interactions between Length and Diameter affecting Weight variables.

### 13. FEEDBACK LOOPS

- Unlikely in this dataset as it represents a static snapshot of abalone characteristics.

### 14. INSTRUMENTAL VARIABLES

- Environmental variables, if available, could serve as instruments to identify causal effects of Age on physical measurements.

### 15. INTERVENTION HISTORY

- No known interventions; the dataset is observational.

### FOR TIME-SERIES DATA (if applicable)

Not applicable as the dataset is cross-sectional.

This analysis provides a structured understanding of the Abalone dataset, focusing on potential causal relationships and domain-specific insights.

We have selected the following algorithm for causal discovery:

Algorithm: PCMCI

Now, we need to determine the optimal hyperparameters for this algorithm. I'll guide you through a systematic approach to select values that prioritize accuracy while maintaining computational efficiency for moderate graph sizes.

Primary hyperparameters to configure: indep_test, tau_min, tau_max, pc_alpha

For each hyperparameter, please follow this structured approach:

Step 1: Understand the dataset characteristics
   - Consider the number of variables (graph size)
   - Analyze sample size and data distribution
   - Note if data is linear/nonlinear, continuous/discrete/mixed
   - For time-series data, prioritize the statistically estimated lag order

Step 2: Assess computational resources
   - Consider the hardware constraints and GPU availability:
   
Current machine doesn't support CUDA, do not choose any GPU-powered algorithms.

   - Prioritize to GPU implemented hyperparameter when dealing with large graph (variable size > 150) to achieve more efficient speedup

Step 3: Evaluate each hyperparameter's impact on accuracy vs. efficiency
   - Critical parameters affecting accuracy (e.g., significance levels, independence tests)
   - Parameters affecting computational complexity (e.g., search depth, maximum conditions)
   - Parameters controlling sparsity (e.g., regularization, thresholds)

Step 4: Analyze algorithm-specific recommendations
   - Review expert suggestions for each parameter:
   "**Parameter:** indep_test\n- **Meaning:** Independence tests\n- **Available Values:**\n  - parcorr\n  - robustparcorr\n  - gpdc\n  - gsq\n  - regression\n  - cmi\n- **Expert Suggestion:** Use parcorr as default. Choose based on data type.\n\nLINEAR/DISCRETE (PARAMETRIC) TESTS:\n- 'parcorr': For linear continuous data (default choice)\n- 'robustparcorr': Robust partial correlation for linear data with outliers\n- 'gsq': For categorical/discrete data only\n\nNONLINEAR/NON-PARAMETRIC TESTS:\n- 'gpdc': For nonlinear continuous data\n- 'cmi': Conditional mutual information for nonlinear data (more accurate but much slower)\n- 'regression': For mixed data types (continuous and discrete)\n\n**Parameter:** tau_min\n- **Meaning:** Minimum time lag to consider\n- **Available Values:**\n  - 0\n  - 1\n- **Expert Suggestion:** Use 0 as default. Adjust if results are required for a specific time window\n\n**Parameter:** tau_max\n- **Meaning:** Maximum time lag\n- **Available Values:**\n  - 1\n  - 5\n  - 10\n- **Expert Suggestion:** Use 1 as default. Usually adjusted based on pre-processing results\n\n**Parameter:** pc_alpha\n- **Meaning:** Significance level of PC algorithm.\n- **Available Values:**\n  - 0.05\n  - 0.1\n  - 0.01\n- **Expert Suggestion:** Use 0.05 as default. Adjust based on sample size, more conservative (lower) values for larger samples. If < 500, use 0.1; Else if 500-10000 (<10000 but >500), use 0.05; Else if > 10000, using 0.01.\n\n"

Step 5: Analyze algorithm performance with different hyperparameter configurations (If existed)
   - Review benchmarking results for this algorithm with various hyperparameter settings
   - Identify which configurations perform best on datasets with similar characteristics
   - Consider how different hyperparameter values affect performance metrics
   - Analyze the trade-offs between accuracy and computational efficiency

# ALGORITHM BENCHMARKING RESULTS

• CAUTIONARY NOTE
  – These benchmarking results should be used as guidelines, not definitive judgments
  – Performance may vary significantly with real-world data compared to simulations
  – Consider your specific domain knowledge and data characteristics when selecting algorithms
• Simulation Settings
  – Network sizes: 5 to 100 nodes
  – Time Lags: 3 to 20 time steps
  – Sample sizes: 500 to 5000 data points
  – Lagged Edge degree: 2.0 to 16.0 (avg 4.0)
  – Instantaneous Edge degree: 0.0 to 4.0 (avg 2.0)
  – Data types: Continuous stationary
  – Function types: Linear relationships
  – Noise types: Gaussian distributions

• Scenario Types
  – Robustness scenarios (e.g., Variable Scaling, Lag Scaling): Test algorithm performance across varying levels of a property
  – Specific scenarios (e.g., Dense Graph, Long time lag): Test performance at a fixed specific setting

• Performance Metrics
  – Performance level (1-10): Based on F1 score, higher is better
  – Efficiency level (0-5): Based on runtime, higher is better (only relevant for scaling scenarios)
  – Stability: Standard deviation of performance, lower values indicate more consistent results

• Important Note on Efficiency Scoring
  – Benchmarks include large-scale systems with up to 100 nodes x 3 lag and may timeout for some algorithms
  – For large-scale systems (node count x time lag > 200), prioritize algorithms that can utilize available GPUs
  – GPU-accelerated methods provide significant efficiency advantages in large-scale scenarios

────────────────────────────────────────────────────────
Filtered Benchmarking Results
────────────────────────────────────────────────────────

Algorithms included: PCMCI

────────────────────────────────────────────────────────
Overall Algorithm Performance
────────────────────────────────────────────────────────

⚠️ IMPORTANT: Overall rankings can be misleading! ⚠️
- An algorithm with high average performance may perform poorly on specific scenarios
- Always check scenario-specific performance for your use case

Overall ranking based on average performance across all scenarios:

1. PCMCI_indep_test=parcorr: 7.4
2. PCMCI_indep_test=regression: 6.8
3. PCMCI_indep_test=robustparcorr: 6.8


────────────────────────────────────────────────────────
Efficiency Comparison
────────────────────────────────────────────────────────

Note: Efficiency scores are primarily measured in Variable Scaling, Lag Scaling and Sample Scaling scenarios.

| Algorithm | Variable Scaling | Sample Scaling | Lag Scaling | Average |
|-----------|------------------|----------------|-------------|---------|
| PCMCI_indep_test=parcorr | 3.1 | 2.2 | 1.0 | 2.1 |


────────────────────────────────────────────────────────
Algorithm Recommendations by Scenario Type
────────────────────────────────────────────────────────

• Linear Relationships
  1. PCMCI_indep_test=parcorr: Performance 7.4
  2. PCMCI_indep_test=regression: Performance 6.8
  3. PCMCI_indep_test=robustparcorr: Performance 6.8

• Dense vs Sparse Graphs
  1. PCMCI_indep_test=parcorr: Performance 7.2
  2. PCMCI_indep_test=regression: Performance 6.8
  3. PCMCI_indep_test=robustparcorr: Performance 6.8


────────────────────────────────────────────────────────
Performance by Scenario
────────────────────────────────────────────────────────

### ROBUSTNESS SCENARIOS
These scenarios test algorithm performance across varying levels of a property.


• Variable Scaling
| Algorithm | Performance | Stability | Efficiency | Overall Score |
|-----------|------------|-----------|------------|-------------|
| PCMCI_indep_test=parcorr | 5.2 | 1.9 | 3.1 | 4.8 |

• Lag Scaling
| Algorithm | Performance | Stability | Efficiency | Overall Score |
|-----------|------------|-----------|------------|-------------|
| PCMCI_indep_test=parcorr | 8.5 | 0.8 | 2.2 | 7.8 |

• Sample Size Scaling
| Algorithm | Performance | Stability | Efficiency | Overall Score |
|-----------|------------|-----------|------------|-------------|
| PCMCI_indep_test=parcorr | 7.1 | 0.4 | 1.0 | 6.2 |
| PCMCI_indep_test=regression | 6.8 | 0.0 | 1.0 | 6.0 |
| PCMCI_indep_test=robustparcorr | 6.8 | 0.0 | 1.0 | 6.0 |

• Lagged Edge Density
| Algorithm | Performance | Stability |
|-----------|------------|----------|
| PCMCI_indep_test=parcorr | 7.8 | 1.0 |
| PCMCI_indep_test=regression | 6.8 | 0.0 |
| PCMCI_indep_test=robustparcorr | 6.8 | 0.0 |

• w Instantaneous Edges
| Algorithm | Performance | Stability |
|-----------|------------|----------|
| PCMCI_indep_test=parcorr | 7.0 | 1.3 |

• w/o Instantaneous Edges
| Algorithm | Performance | Stability |
|-----------|------------|----------|
| PCMCI_indep_test=parcorr | 6.7 | 0.6 |

### SPECIFIC SCENARIOS
These scenarios test algorithm performance at specific settings rather than variable levels.


• Dense Graph
| Algorithm | Performance | Stability |
|-----------|------------|----------|
| PCMCI_indep_test=parcorr | 7.8 | 0.0 |

• Long Time Lag
| Algorithm | Performance | Stability |
|-----------|------------|----------|
| PCMCI_indep_test=parcorr | 8.7 | 0.0 |

• Large Sample Size
| Algorithm | Performance | Stability |
|-----------|------------|----------|
| PCMCI_indep_test=parcorr | 7.8 | 0.0 |


-------------------------------------------

Step 6: Make final decisions based on:
   - For moderate graph sizes (<50 variables), prioritize accuracy over speed
   - For large graphs (>50 variables), balance accuracy with feasibility and EFFICIENCY
   - For time-series data, carefully consider temporal parameters

Please provide your suggestions in a structured JSON format, with detailed reasoning for each hyperparameter. Your response should look like this:

{
  "algorithm": "PCMCI",
  "hyperparameters": {
    "[HYPERPARAMETER_1_NAME]": {
      "full_name": "[HYPERPARAMETER_1_FULL_NAME]",
      "reasoning": "[YOUR_STEP_BY_STEP_REASONING_PROCESS]",
      "value": [SUGGESTED_VALUE],
      "explanation": "[BRIEF_EXPLANATION_OF_TRADEOFFS]"
    },
    "[HYPERPARAMETER_2_NAME]": {
      "full_name": "[HYPERPARAMETER_2_FULL_NAME]",
      "reasoning": "[YOUR_STEP_BY_STEP_REASONING_PROCESS]",
      "value": [SUGGESTED_VALUE],
      "explanation": "[BRIEF_EXPLANATION_OF_TRADEOFFS]"
    }
  }
}

Important guidelines:
1. Only select values from the "available_values" list for each hyperparameter
2. For moderate graph sizes (10-50 variables), prioritize accuracy over speed
3. For time-series data, give special attention to lag parameters based on statistical estimates
4. For time-series data of moderate size (10-50) variables, if the estimated time lag is small (1 or 2), consider using a larger value
5. Consider independence test selection carefully based on data type and computational resources
6. For regularization parameters, consider the expected graph density
7. For search depth parameters, consider the complexity of potential causal relationships

Please provide your hyperparameter suggestions following this JSON structure, with clear reasoning that demonstrates you've considered the dataset characteristics, algorithm requirements, and computational constraints.